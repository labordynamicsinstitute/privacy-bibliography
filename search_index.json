[["index.html", "Introductory Readings in Formal Privacy for Economists 1 Overview 1.1 Contributing 1.2 Acknowledgements 1.3 Disclaimer", " Introductory Readings in Formal Privacy for Economists John M. Abowd, Ian M. Schmutte, William Sexton, and Lars Vilhuber 2020-10-11 1 Overview The purpose of this document is to provide scholars with a comprehensive list of readings relevant to the economic analysis of formal privacy, and particularly its application to public statistics. Statistical agencies and tech giants are rapidly adopting formal privacy models which make the tradeoff between privacy and data quality precise. The question then becomes, how much privacy loss should they allow? J. M. Abowd &amp; Schmutte (2019) argue that this choice ultimately depends on how decision makers weigh the costs of privacy loss against the benefits of higher-quality data. Making progress on these questions requires familiarity with new tools from computer science and statistics, the objectives and policy environment within which statistical agencies operate, along with the economic analysis of information. We have organized these references into a reading course focused on 10-15 primary references in each of six different topics: Formal Privacy Policy and Official Statistics Statistical Disclosure Limitation Economics of Privacy Value of Privacy and Data Accuracy In the remainder of this document, for each topic, we provide a very brief description of the papers in the reading course and why we selected them. In each case, we orient the reader to the key issues, concepts, and tools in each topic. All references are repeated in the global References section. In addition to this reading course, we have also curated a much more extensive list of references, available here. 1.1 Contributing We encourage interested readers and researchers to use these readings for their classes and training, modifying it as needed. You can fork the source code at https://github.com/labordynamicsinstitute/privacy-bibliography. This document is licensed under the Creative Commons Attribution-NonCommercial 4.0 International Public License (CC BY-NC 4.0). Should that license not meet your needs, contact us to discuss possible modifications or exemption. Please send us any improvements, corrections, or other contributions, either by e-mail, or via the &quot;Issues&quot; feature on Github, or by clicking on the edit button in the toolbar above. 1.2 Acknowledgements We gratefully acknowledge the support of Alfred P. Sloan Foundation Grant G-2015-13903 and NSF Grant SES-1131848. 1.3 Disclaimer The views expressed in this paper are those of the authors and not those of the U.S. Census Bureau or other sponsors. References "],["background.html", "2 Background", " 2 Background We start by providing a short list of background references that frame a particular set of research topics. J. M. Abowd &amp; Schmutte (2019) show how to combine formal privacy models with the classic theory of public goods to understand and guide decisions about privacy protection and data dissemination. For the neophyte, Wood et al. (2018) provide a non-technical introduction to formal privacy models in general, and differential privacy in particular. Heffetz &amp; Ligett (2014) also introduce differential privacy, but targeted toward economists. After reading these introductory treatments, you should review the textbook treatment of differential privacy in Dwork &amp; Roth (2014) , focusing on Chapters 1--3. We also recommend consultation of the very fine tutorial &quot;Differential Privacy in the Wild: A tutorial on current practices and open challenges&quot; by Michael Hay, Xi He, and Ashwin Machanavajjhala. The tutorial is available in two parts. Part 1: (slides, video) Part 2: (slides, video) To provide a concrete grounding in practical issues of privacy, Jones (2017) summarizes the history of privacy breaches and privacy protection in the U.S. statistical system. It is important to ask how formal privacy models do, and do not, capture common language and legal interpretations of privacy. As such, we also recommend a review of some of the laws governing data privacy in the U.S. , e.g. 13 U.S. Code (1954) and H.R.4174 (2018) (Confidential Information Protection and Statistical Efficiency Act, also known as CIPSEA). A quick review of the Harvard Privacy Tools website (Harvard University Privacy Tools Project, 2019) can provide a sense of how differential privacy is being implemented in various settings. Goroff (2015) also provides a very useful overview of the key issues in this reading course for a lay audience. J. M. Abowd (2017) and J. M. Abowd (2016) survey formal privacy systems being implemented at the Census Bureau. References "],["formal-privacy.html", "3 Formal Privacy", " 3 Formal Privacy The literature on formal privacy is vast. Here, we focus on those papers that will help orient the reader to the key ideas and tools of differential privacy, particularly those relevant to the economic problem of determining optimal privacy protection when publishing population statistics. For the purpose of this reading course and associated bibliography, we associate formal privacy as a literature emerging in computer science out of cryptography. In the section on &quot;Statistical Disclosure Limitation&quot;, we recommend additional readings from the SDL literature, which has a distinct origin in statistics. Dwork, McSherry, Nissim, &amp; Smith (2006) is generally regarded as the first paper to formally introduce differential privacy. Its development was due, in part, to the database reconstruction theorem published by Dinur &amp; Nissim (2003), which showed that most data publication mechanisms are blatantly non-private in a well-defined sense. The database reconstruction theorem has recently been shown to have very practical consequences for the statistical disclosure protections in place at the Census Bureau. The concepts of k-anonymity (Sweeney, 2002) and l-diversity (Ashwin Machanavajjhala, Kifer, Gehrke, &amp; Venkitasubramaniam, 2007) are important antecedents that bridge the formal privacy and SDL literatures. In assessing formal privacy as a framework for modeling data publication, it is natural to consider the optimal, or maximal amount of data accuracy that can be provided while maintaining privacy. Gupta, Roth, &amp; Ullman (2012) establish a mechanism that is universally optimal for a broad class of data users, suggesting that technical efficiency could be guaranteed in private data publication without the need to learn about the preferences or side information of data consumers. However, Brenner &amp; Nissim (2014) showed that their result is not generalizable to broader types of data publication. Nissim, Orlandi, &amp; Smorodinsky (2012) provide a clear and instructive illustration of how individual preferences for privacy can be modeled in mechanism design problems. Several papers are more directly relevant to understanding how privacy affects the work of statistical agencies. Cummings, Echenique, &amp; Wierman (2014) argue that privacy concerns can affect the way people report data, and show how, if properly designed, privacy protection may mitigate misreporting. While there are vast number of papers on the implementation of differentially private publication algorithms, a few are particularly relevant to how statistical agencies operate. Hardt, Ligett, &amp; McSherry (2012) and Hardt &amp; Rothblum (2010) provide the methods and theory for publication through online query systems. One problem with these methods is that their implementation depends on the underlying data. By contrast, C. Li, Miklau, Hay, McGregor, &amp; Rastogi (2015) introduce the Matrix Mechanism, which is data-independent, but also can provide high accuracy for reasonable levels of privacy. This is one of the methods under development for use with the 2020 Decennial Census. Other formal privacy systems currently in use at Census are documented in A. Machanavajjhala, Kifer, Abowd, Gehrke, &amp; Vilhuber (2008) and Haney et al. (2017). For practitioners, Ashwin Machanavajjhala &amp; Kifer (2015) provide an accessible and very useful overview of how formal privacy methods might be applied to real-world data publishing tasks. Finally, so-called &quot;privacy semantics&quot; are mappings between mathematical definitions of privacy and plain language. These are really important for practitioners because there is usually a gap between how laypeople think about privacy and how it is defined in the CS literature. By way of introduction, we recommend He, Machanavajjhala, &amp; Ding (2014), Jorgensen, Yu, &amp; Cormode (2015). References "],["policy-and-official-statistics.html", "4 Policy and Official Statistics", " 4 Policy and Official Statistics In January 2017, the Committee on National Statistics released a special panel report focused on developing innovations in the U.S. statistical system focused, in part, on preserving privacy (National Academies of Sciences, Engineering, and Medicine, 2017). Their report is essential reading to understand the governing principles and practical needs of the statistical system, particularly as it relates to privacy modernization. For a more applied perspective, Schmutte &amp; Vilhuber (2017) report the proceedings of an ad hoc workshop on practical privacy held at the Census Bureau. That workshop gathered together academic privacy researchers and Census domain experts to help design formal privacy systems for key data products. In such meetings, it is necessary to make sure people are speaking the same language. Prewitt (2011) describes the specific meanings of the terms &quot;privacy&quot; and &quot;confidentiality&quot; as they have historically been used at the Census Bureau. Manski (2015) offers a framework for thinking about total error in official statistics, which refers to the various ways measured quantities may differ from the concepts of interest, including measurement error, design error, and sampling error. From this perspective, privacy protection is yet another source of error in any statistical system. Maintaining the public trust is a key factor motivating the interest of statistical agencies in privacy protection. The less people trust the system, the less likely they respond accurately, or at all. Childs, King, &amp; Fobia (2015) discuss recent statistics on trust in official statistics and their implications for data collection. Finally, Haney et al. (2017) and Holan, Toth, Ferreira, &amp; Karr (2010) are good examples of the sorts of implementation details one may encounter when applying statistical privacy protections in public data. References "],["statistical-disclosure-limitation.html", "5 Statistical Disclosure Limitation", " 5 Statistical Disclosure Limitation Within most national statistical systems, the primary approach to protecting respondent privacy has been statistical disclosure limitation or SDL. Anderson &amp; Seltzer (2007) describes the history of threats to confidentiality in the U.S. statistical system prior to 1965. Fellegi (1972) initiated the statistical analysis of data confidentiality. Dalenius (1977) recognized that statistical agencies would need to do more than just protect against direct disclosures, and thus warned against what he called inferential disclosure. His idea was formalized by Duncan &amp; Lambert (1986), and provides the ultimate rationale for formal privacy in national statistics. J. M. Abowd &amp; Schmutte (2015) review the SDL methods currently in use and discuss their application to economic data. They argue that the analysis of SDL-laden data is inherently compromised because the details of the SDL protections cannot be disclosed. If they cannot be disclosed, their consequences for inference are unknowable, and, as they show, potentially large. Garfinkel (2015) discusses techniques for de-identifying data and the many ways in which modern computing tools and a data-rich environment may render effective de-identification impossible. Finally, Harris-Kojetin et al. (2005) provides the most comprehensive review of SDL methods currently in use across the U.S. statistical system. References "],["economics-of-privacy.html", "6 Economics of Privacy", " 6 Economics of Privacy There is a large and robust literature on privacy in economics. That literature is generally focused on the value to individuals of being able to conceal private information, like a health condition, from a firm or prospective employer. The challenge to the firm is to design mechanisms, like pricing strategies, that encourage people to disclose private information. For an overview of ideas in this literature, we recommend reading Stigler (1980), Posner (1981), and Hirshleifer (1980), which appeared in an early symposium. Varian (2002) and Acquisti, Taylor, &amp; Wagman (2016) both provide comprehensive surveys at different points in the development of this literature. Very few papers tie the economics of privacy directly to official statistics. Campbell, Goldfarb, &amp; Tucker (2015) discuss the consequences for firm profits and individual welfare of data privacy restrictions in California, which prevent firms from sharing certain types of mortgage data. Goldfarb &amp; Tucker (2012) discuss shifts in privacy attitudes and their implications for firms. Hsu et al. (2014), address the question of optimal privacy protection from a very similar perspective to J. M. Abowd &amp; Schmutte (2019). Finally, Ohm (2010) surveys the economic implications of contemporary threats to data privacy from a legal perspective. References "],["value-of-privacy-and-data-accuracy.html", "7 Value of Privacy and Data Accuracy", " 7 Value of Privacy and Data Accuracy One key challenge for implementing formal privacy systems lies in choosing the amount, or type, of privacy to provide. Answering this question requires some way to understand the individual and social value of privacy. Ghosh &amp; Roth (2015) and C. Li, Li, Miklau, &amp; Suciu (2014) both model mechanisms for pricing private data under the assumption that individuals are only willing to disclose such information if they are paid. Part of the social value of privacy arises from its relationship to scientific integrity. While the law of information recovery suggests that improved privacy must come at the cost of increased error in published statistics, these effects might be mitigated through two distinct channels. First, people are more truthful in surveys if they believe their data is not at risk, as Couper, Singer, Conrad, &amp; Groves (2008) illustrate. Second, work in computer science (Dwork et al., 2015 ) and statistics (Cummings, Ligett, Nissim, Roth, &amp; Wu, 2016) suggests another somewhat surprising benefit of differential privacy: protection against overfitting. A complete accounting of the costs and benefits of formal privacy systems should take these channels into account. It is equally necessary to develop a more robust understanding of why data is valuable in the first place, the overall social cost of increasing error in public statistics. This seems to be an area in which very little comprehensive theoretical or empirical research has been done. We nevertheless recommend what seem to be good starting points. On the theoretical side, economists studying privacy have also developed models of the value of data to firms. In these models, firms benefit from being able to tailor prices based on individual demand (C. R. Taylor, 2004), or from being able to market more effectively (Varian, 1998). More recently, a theoretical literature on information design has begun to consider more effective ways to manage markets for consumer information, see Bergemann, Bonatti, &amp; Smolin (2018) and Pomatto, Strack, &amp; Tamuz (2018). The recent literature is related to Bruce D. Spencer (1985), who developed a decision-theoretic framework for modeling optimal data quality. On the empirical side, a handful of interesting use cases suggest techniques for uncovering the value of data. For example, Card, Mas, Moretti, &amp; Saez (2012) and Perez-Truglia (2016) show how workers respond to pay transparency policies, which give them new information about co-worker salaries. Bruce David Spencer &amp; Seeskin (2015) use a calibration exercise to study the costs, measured in misallocated congressional seats, of reduced accuracy in population census data. References "],["references.html", "8 References", " 8 References "]]
